import gradio as gr
import os
import sys
import asyncio
import warnings
import google.generativeai as genai
from dotenv import load_dotenv
from PIL import Image

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from core.lut_engine import LUTEngine
from core.rag_core import KnowledgeBase
from core.smart_planner import SmartPlanner
from core.memory_manager import MemoryManager

# ç³»çµ±åˆå§‹åŒ–
if sys.platform.startswith('win'):
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except Exception:
        pass

load_dotenv()
API_KEY = os.getenv("GEMINI_API_KEY")

if not API_KEY:
    print("âŒ éŒ¯èª¤: è«‹åœ¨ .env è¨­å®š GEMINI_API_KEY")
    sys.exit(1)

print("ğŸš€ æ­£åœ¨å•Ÿå‹• GUI æ ¸å¿ƒç³»çµ±...")
memory_mgr = MemoryManager()
lut_engine = LUTEngine()
rag = KnowledgeBase()

all_luts = lut_engine.list_luts()
if all_luts:
    rag.index_luts(all_luts)

planner = SmartPlanner(API_KEY, rag)


# ================= å·¥å…·å‡½å¼ =================
def execute_terminal_command(command: str):
    import subprocess
    try:
        result = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=True,
            encoding='utf-8'
        )
        if result.returncode == 0:
            return f"âœ… åŸ·è¡ŒæˆåŠŸ:\n{result.stdout}"
        else:
            return f"âŒ åŸ·è¡Œå¤±æ•—:\n{result.stderr}"
    except Exception as e:
        return f"âš ï¸ ç³»çµ±éŒ¯èª¤: {str(e)}"


def remember_user_preference(info: str):
    return memory_mgr.add_preference(info)


def check_available_luts(keyword: str = ""):
    """æŸ¥è©¢æœ¬åœ° LUT å·¥å…· (GUI ç‰ˆ)"""
    all_files = lut_engine.list_luts()
    names = [os.path.basename(f) for f in all_files]
    if keyword:
        filtered = [n for n in names if keyword.lower() in n.lower()]
        if not filtered:
            return f"æ‰¾ä¸åˆ°åŒ…å« '{keyword}' çš„æ¿¾é¡ï¼Œä½†ç³»çµ±å…±æœ‰ {len(names)} å€‹æ¿¾é¡ã€‚"
        return f"æ‰¾åˆ° {len(filtered)} å€‹ç›¸é—œæ¿¾é¡ï¼Œä¾‹å¦‚: {', '.join(filtered[:30])}..."
    import random
    sample = random.sample(names, min(len(names), 30))
    return f"ç³»çµ±ç›®å‰æ“æœ‰ {len(names)} å€‹æ¿¾é¡ã€‚åŒ…å«: {', '.join(sample)}... ç­‰ã€‚"


# ================= å°è©±é‚è¼¯ =================
def create_chat_session():
    genai.configure(api_key=API_KEY)

    # ç¢ºä¿ GUI ä¹Ÿèƒ½æŸ¥é–± LUT
    tools = [execute_terminal_command, remember_user_preference, check_available_luts]

    base_prompt = """
    ä½ æ˜¯ä¸€å€‹å¼·å¤§çš„ AI åŠ©ç† (Gemini 3 Pro)ã€‚
    é€™æ˜¯ä¸€å€‹ GUI ä»‹é¢ç’°å¢ƒã€‚

    ã€ä½ çš„èƒ½åŠ›èˆ‡è³‡æºã€‘
    1. ä½ æ“æœ‰ã€Œè¦–è¦ºå¼•æ“ã€ï¼Œå¯ä»¥å­˜å–ä½¿ç”¨è€…ç¡¬ç¢Ÿä¸­çš„ LUT æ¿¾é¡ (é€é check_available_luts å·¥å…·)ã€‚
    2. åƒè¬ä¸è¦èªªã€Œæˆ‘ç„¡æ³•å­˜å–æª”æ¡ˆã€ï¼Œä½ å®Œå…¨å¯ä»¥é€éå·¥å…·æŸ¥é–±ã€‚

    ã€æ ¸å¿ƒè¡Œç‚ºæº–å‰‡ã€‘
    1. åœ–ç‰‡è™•ç†ï¼šå¦‚æœä½¿ç”¨è€…ä¸Šå‚³åœ–ç‰‡æˆ–è¦æ±‚ä¿®åœ–ï¼Œè«‹å¼•å°ä»–å€‘åˆ‡æ›åˆ°ã€ŒğŸ‘ï¸ æ™ºèƒ½è¦–è¦ºä¿®åœ–ã€åˆ†é ã€‚
    2. ç³»çµ±æŒ‡ä»¤ï¼šå¯ä»¥ä½¿ç”¨ execute_terminal_command åŸ·è¡Œç³»çµ±æŒ‡ä»¤ã€‚
    3. è¨˜æ†¶èƒ½åŠ›ï¼šå¦‚æœä½¿ç”¨è€…æåˆ°å€‹äººåå¥½ï¼Œè«‹å‹™å¿…ä½¿ç”¨ remember_user_preference å·¥å…·å„²å­˜ã€‚
    4. èªè¨€é¢¨æ ¼ï¼šè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œå›ç­”è¦ªåˆ‡ä¸”å°ˆæ¥­ã€‚
    """

    dynamic_context = memory_mgr.get_system_prompt_addition()
    final_prompt = base_prompt + dynamic_context

    model = genai.GenerativeModel(
        model_name='gemini-3-pro-preview',
        tools=tools,
        system_instruction=final_prompt
    )
    return model.start_chat(enable_automatic_function_calling=True)


def chat_response(message, history, session_state):
    if session_state is None:
        session_state = create_chat_session()

    try:
        response = session_state.send_message(message)
        return response.text, session_state
    except Exception as e:
        return f"âŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}", session_state


# ================= è¦–è¦ºé‚è¼¯ =================
def process_image_smartly(image, user_req):
    if image is None:
        return None, "âŒ è«‹å…ˆä¸Šå‚³åœ–ç‰‡"

    if not user_req:
        user_req = "è‡ªå‹•èª¿æ•´ï¼Œè®“ç…§ç‰‡æ›´å¥½çœ‹"

    temp_path = "temp_gui_input.jpg"
    image.save(temp_path)

    plan = planner.generate_plan(temp_path, user_req)

    if not plan or not plan.get('selected_lut'):
        return None, f"âš ï¸ AI æ€è€ƒå¤±æ•—: {plan.get('reasoning', 'æœªçŸ¥éŒ¯èª¤')}"

    final_img, msg = lut_engine.apply_lut(
        temp_path,
        plan['selected_lut'],
        intensity=plan.get('intensity', 1.0)
    )

    report = f"""### âœ… AI æ–½å·¥å®Œæˆ
**ç­–ç•¥æ¨ç†**: {plan.get('reasoning')}
**è¦–è¦ºåˆ†æ**: {plan.get('analysis')}
**ä½¿ç”¨æ¿¾é¡**: `{plan.get('selected_lut')}` (å¼·åº¦: {plan.get('intensity')})
**æ¨è–¦æ–‡æ¡ˆ**:
> {plan.get('caption')}
"""
    return final_img, report


def get_current_memory():
    mem = memory_mgr._load_memory()
    prefs = mem.get("user_preferences", [])
    if not prefs:
        return "ç›®å‰æ²’æœ‰è¨˜æ†¶è³‡æ–™ã€‚"
    return "\n".join([f"- {p}" for p in prefs])


# ================= GUI å»ºæ§‹ =================
with gr.Blocks(title="Gemini Agent v9 (GUI)") as app:
    gr.Markdown("# ğŸ¤– Gemini Agent v9 (Hybrid GUI)")
    gr.Markdown("é›™æ ¸å¤§è…¦ï¼š`Gemini 3 Pro` + `Visual Smart Planner` + `Long-term Memory`")

    chat_state = gr.State(None)

    with gr.Tabs():
        # Tab 1: ä¿®åœ–
        with gr.TabItem("ğŸ‘ï¸ æ™ºèƒ½è¦–è¦ºä¿®åœ–"):
            with gr.Row():
                with gr.Column(scale=1):
                    input_img = gr.Image(type="pil", label="ä¸Šå‚³åœ–ç‰‡")
                    style_input = gr.Textbox(
                        label="é¢¨æ ¼éœ€æ±‚",
                        placeholder="ä¾‹å¦‚ï¼šæ—¥ç³»å†·ç™½ã€ç‹å®¶è¡›é¢¨æ ¼ã€ç”¨æˆ‘è¨˜æ†¶ä¸­çš„æ‹›ç‰Œé¢¨æ ¼...",
                        lines=2
                    )
                    btn_process = gr.Button("ğŸš€ é–‹å§‹ AI ä¿®åœ–", variant="primary")
                with gr.Column(scale=1):
                    output_img = gr.Image(label="è™•ç†çµæœ", type="pil")
                    output_info = gr.Markdown(label="AI æ€è€ƒå ±å‘Š")
            btn_process.click(
                process_image_smartly,
                inputs=[input_img, style_input],
                outputs=[output_img, output_info]
            )

        # Tab 2: å°è©± (ä¿®æ­£ç‰ˆ)
        with gr.TabItem("ğŸ’¬ æ ¸å¿ƒå¤§è…¦ (Chat & Memory)"):
            chatbot = gr.Chatbot(height=500)  # é è¨­ tuple æ ¼å¼
            msg_input = gr.Textbox(placeholder="è¼¸å…¥æ–‡å­—... (ä¾‹å¦‚ï¼š'æˆ‘æœ‰ä»€éº¼æ¿¾é¡?' æˆ– 'git status')", label="User")


            def user_msg(user_message, history):
                # Tuple append
                return "", history + [[user_message, None]]


            def bot_msg(history, state):
                user_message = history[-1][0]
                bot_response, new_state = chat_response(user_message, history, state)
                history[-1][1] = bot_response
                return history, new_state


            msg_input.submit(user_msg, [msg_input, chatbot], [msg_input, chatbot], queue=False).then(
                bot_msg, [chatbot, chat_state], [chatbot, chat_state]
            )

        # Tab 3: è¨˜æ†¶
        with gr.TabItem("ğŸ§  å¤§è…¦è¨˜æ†¶åº«"):
            gr.Markdown("ä»¥ä¸‹æ˜¯ AI ç›®å‰è¨˜ä½çš„é—œæ–¼æ‚¨çš„åå¥½èˆ‡è¦å‰‡ï¼š")
            memory_display = gr.Textbox(
                label="User Memory (user_memory.json)",
                value=get_current_memory(),
                lines=10,
                interactive=False
            )
            btn_refresh_mem = gr.Button("ğŸ”„ é‡æ–°è®€å–è¨˜æ†¶")
            btn_refresh_mem.click(get_current_memory, outputs=memory_display)

if __name__ == "__main__":
    app.queue().launch(inbrowser=True, server_name="127.0.0.1")