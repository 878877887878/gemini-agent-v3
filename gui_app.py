import gradio as gr
import os
import sys
import asyncio
import warnings
import google.generativeai as genai
from dotenv import load_dotenv
from PIL import Image

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from core.lut_engine import LUTEngine
from core.rag_core import KnowledgeBase
from core.smart_planner import SmartPlanner
from core.memory_manager import MemoryManager
from core.security import execute_safe_command
from core.logger import Logger

if sys.platform.startswith('win'):
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except Exception:
        pass

load_dotenv()
API_KEY = os.getenv("GEMINI_API_KEY")

if not API_KEY:
    print("âŒ éŒ¯èª¤: è«‹åœ¨ .env è¨­å®š GEMINI_API_KEY")
    sys.exit(1)

Logger.info("æ­£åœ¨å•Ÿå‹• GUI æ ¸å¿ƒç³»çµ± (v13 Cinematic)...")
memory_mgr = MemoryManager()
lut_engine = LUTEngine()
rag = KnowledgeBase()

all_luts = lut_engine.list_luts()
if all_luts:
    rag.index_luts(all_luts)

planner = SmartPlanner(API_KEY, rag)


# å·¥å…·å‡½å¼
def remember_user_preference(info: str):
    Logger.info(f"GUI è§¸ç™¼è¨˜æ†¶å¯«å…¥: {info}")
    return memory_mgr.add_preference(info)


def check_available_luts(keyword: str = ""):
    all_names = list(lut_engine.lut_index.keys())
    if keyword:
        filtered = [n for n in all_names if keyword.lower() in n]
        if not filtered:
            return f"æ‰¾ä¸åˆ°åŒ…å« '{keyword}' çš„æ¿¾é¡ã€‚"
        return f"æ‰¾åˆ° {len(filtered)} å€‹ç›¸é—œæ¿¾é¡..."
    return f"ç³»çµ±ç›®å‰æ“æœ‰ {len(all_names)} å€‹æ¿¾é¡ã€‚"


# å°è©±é‚è¼¯
def create_chat_session():
    genai.configure(api_key=API_KEY)
    tools = [execute_safe_command, remember_user_preference, check_available_luts]
    base_prompt = """
    ä½ æ˜¯ä¸€å€‹å¼·å¤§çš„ AI åŠ©ç† (Gemini 3 Pro)ã€‚
    ã€è¡Œç‚ºæº–å‰‡ã€‘å¼•å°ä½¿ç”¨åœ–ç‰‡æ¨¡å¼ã€åŸ·è¡Œç™½åå–®æŒ‡ä»¤ã€è¨˜æ†¶åå¥½ã€‚
    """
    dynamic_context = memory_mgr.get_system_prompt_addition()
    model = genai.GenerativeModel(
        model_name='gemini-3-pro-preview',
        tools=tools,
        system_instruction=base_prompt + dynamic_context
    )
    return model.start_chat(enable_automatic_function_calling=True)


def chat_response(message, history, session_state):
    if session_state is None:
        session_state = create_chat_session()
    try:
        Logger.debug(f"GUI å°è©±è«‹æ±‚: {message}")
        response = session_state.send_message(message)
        return response.text, session_state
    except Exception as e:
        Logger.error(f"GUI å°è©±éŒ¯èª¤: {e}")
        return f"âŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}", session_state


# ================= è¦–è¦ºé‚è¼¯ (v13 Update) =================
def process_image_smartly(image, user_req):
    Logger.info(f"GUI è§¸ç™¼ä¿®åœ–ï¼Œéœ€æ±‚: {user_req}")
    if image is None: return None, "âŒ è«‹å…ˆä¸Šå‚³åœ–ç‰‡"
    if not user_req: user_req = "è‡ªå‹•èª¿æ•´"

    temp_path = "temp_gui_input.jpg"
    image.save(temp_path)

    plan = planner.generate_plan(temp_path, user_req)

    if not plan or not plan.get('selected_lut'):
        return None, f"âš ï¸ AI æ€è€ƒå¤±æ•—: {plan.get('reasoning', 'æœªçŸ¥éŒ¯èª¤')}"

    # v13 å‚³éæ‰€æœ‰æ–°åƒæ•¸ (å« Curve/Sharpness)
    final_img, msg = lut_engine.apply_lut(
        temp_path,
        plan['selected_lut'],
        intensity=plan.get('intensity', 1.0),
        brightness=plan.get('brightness', 1.0),
        saturation=plan.get('saturation', 1.0),
        temperature=plan.get('temperature', 0.0),
        tint=plan.get('tint', 0.0),
        contrast=plan.get('contrast', 1.0),
        curve=plan.get('curve', 'Linear'),  # æ–°å¢
        sharpness=plan.get('sharpness', 1.0)  # æ–°å¢
    )

    # v13 å°ˆæ¥­å ±å‘Š
    report = f"""### ğŸ¨ AI èª¿è‰²å¸«å ±å‘Š (v13)
**æŠ€è¡“åˆ†æ**: {plan.get('technical_analysis', 'ç„¡')}
**èª¿è‰²ç­–ç•¥**: {plan.get('style_strategy', 'ç„¡')}

| åƒæ•¸é¡åˆ¥ | è¨­å®šå€¼ |
| :--- | :--- |
| **LUT** | `{plan.get('selected_lut')}` (å¼·åº¦ {plan.get('intensity')}) |
| **è‰²å½©å¹³è¡¡** | Temp: `{plan.get('temperature')}` / Tint: `{plan.get('tint')}` |
| **æ›å…‰è³ªæ„Ÿ** | Curve: `{plan.get('curve')}` / Bright: `{plan.get('brightness')}` |
| **ç´°ç¯€** | Sharpness: `{plan.get('sharpness')}` / Contrast: `{plan.get('contrast')}` |

> {plan.get('caption')}
"""
    return final_img, report


def get_current_memory():
    mem = memory_mgr._load_memory()
    prefs = mem.get("user_preferences", [])
    if not prefs: return "ç›®å‰æ²’æœ‰è¨˜æ†¶è³‡æ–™ã€‚"
    return "\n".join([f"- {p}" for p in prefs])


# GUI å»ºæ§‹
with gr.Blocks(title="Gemini Agent v13 (Cinematic)") as app:
    gr.Markdown("# ğŸ¤– Gemini Agent v13 (Cinematic Grade)")
    gr.Markdown("å¼•æ“ç‰¹è‰²ï¼š`Log LUT é˜²å‘†` + `S-Curve é›»å½±æ›²ç·š` + `Tint è†šè‰²æ ¡æ­£`")

    chat_state = gr.State(None)

    with gr.Tabs():
        with gr.TabItem("ğŸ‘ï¸ æ™ºèƒ½è¦–è¦ºä¿®åœ–"):
            with gr.Row():
                with gr.Column(scale=1):
                    input_img = gr.Image(type="pil", label="ä¸Šå‚³åœ–ç‰‡")
                    style_input = gr.Textbox(label="é¢¨æ ¼éœ€æ±‚", placeholder="æ—¥ç³»å†·ç™½ã€é›»å½±æ„Ÿ...", lines=2)
                    btn_process = gr.Button("ğŸš€ é–‹å§‹ v13 ä¿®åœ–", variant="primary")
                with gr.Column(scale=1):
                    output_img = gr.Image(label="è™•ç†çµæœ", type="pil")
                    output_info = gr.Markdown(label="AI æ€è€ƒå ±å‘Š")
            btn_process.click(
                process_image_smartly,
                inputs=[input_img, style_input],
                outputs=[output_img, output_info]
            )

        with gr.TabItem("ğŸ’¬ æ ¸å¿ƒå¤§è…¦"):
            chatbot = gr.Chatbot(height=500)
            msg_input = gr.Textbox(label="User", placeholder="èŠå¤©æˆ–æŒ‡ä»¤...")


            def user_msg(user_message, history):
                return "", history + [[user_message, None]]


            def bot_msg(history, state):
                user_message = history[-1][0]
                bot_response, new_state = chat_response(user_message, history, state)
                history[-1][1] = bot_response
                return history, new_state


            msg_input.submit(user_msg, [msg_input, chatbot], [msg_input, chatbot], queue=False).then(
                bot_msg, [chatbot, chat_state], [chatbot, chat_state]
            )

        with gr.TabItem("ğŸ§  è¨˜æ†¶åº«"):
            memory_display = gr.Textbox(label="User Memory", value=get_current_memory(), lines=10, interactive=False)
            btn_refresh = gr.Button("ğŸ”„ é‡æ–°è®€å–")
            btn_refresh.click(get_current_memory, outputs=memory_display)

if __name__ == "__main__":
    app.queue().launch(inbrowser=True, server_name="127.0.0.1")